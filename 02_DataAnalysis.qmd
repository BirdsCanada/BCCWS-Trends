---
title: "02_DataAnalysis"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

# 2 Data Analysis

Run setup if starting from a clean environment.

```{r}

source("Scripts/00_Setup.R")

#Manually specify the area of the analysis. In this case it will be BCCWS
area <- "BCCWS"
#Specify the model
model<-"iCAR"

#Manually Specify the start and end year of the analysis
Y1 = 1999
Y2 = 2024

#Read in the clean datasets
sp.data<-read.csv("Data/sp.data.csv", row.names = 1)
sp.data <- sp.data %>% filter(wyear >= Y1 & wyear <= Y2, row.names=1)

events<-read.csv("Data/BCCWS.events.csv")
events <- events %>% filter(wyear >= Y1 & wyear <= Y2)

#Load the Salish Sea Water Polygon for graphing results 
map<- st_read("Data/Spatial/UpdatesAreaPlanLayers.shp")

#ensure the coordinate reference systems are the same
mapCRS<-st_crs(map) #get the CRS of the poly data

#ensure the coordinate reference systems are the same
mapCRS<-st_crs(map) #get the CRS of the poly data

#Create a spatial points layers for graphing results
loc<-events %>% select(SurveyAreaIdentifier, DecimalLatitude, DecimalLongitude) %>% distinct()
xy<-st_as_sf(loc, coords = c("DecimalLongitude", "DecimalLatitude"))
st_crs(xy)<-"+proj=longlat +datum=NAD83"
xy<-st_transform(xy, mapCRS) #transform 

```

## 2.1 Set Analysis Parameters 

A polygon should have counts in all years to ensure that sampling is complete and consistent. Those that have incomplete time series are removed.


```{r}
#  assignment_method <- "within"  
 assignment_method <- "nearest"
  
# Load and prepare polygon
  poly <- st_read("Data/Spatial/UpdatesAreaPlanLayers.shp") %>% 
    st_make_valid() %>% 
    st_transform(st_crs(mapCRS))  # Ensure CRS matches points
  
  # Define the names of the polygons you want to combine
  areas_to_combine <- c("North Vancouver Island", "North Coast", "Central Coast", 
                      "Haida Gwaii", "West Coast Vancouver Island")
 #List available fields for user selection
  print(names(poly))
  
# Create a new grouping variable and then combine the polygons
  poly <- poly %>%
  mutate(group_name = ifelse(Area %in% areas_to_combine, 
                             "Combined Outer Coast", 
                             Area)) %>%
  group_by(group_name) %>%
  summarise(geometry = st_union(geometry))

  # You can now view the new, combined sf object
  print(poly)

#Prompt user to select polygon ID and area fields
  polygon_id_field <- "group_name" #Enter the field name for polygon ID (e.g., 'WatershedID')
  area_field <- "" #Enter the field name for area (or leave blank if none or you desire equal weights)

#Rename fields to 'Name' and 'Area'. If polygon area is missing, this will be created and assigned to 1. Analytically, this means the "Study Area" trend will be equally weighted across all polygons. 
  poly <- poly %>%
  rename(Name = !!polygon_id_field) %>%
  mutate(Area = if (area_field != "") .data[[area_field]] else 1)
  
#Assignment method   
 if (assignment_method == "within") {
  # Assign points to polygons only if strictly within
  xy_assigned <- st_join(xy, poly, join = st_within)
} else if (assignment_method == "nearest") {
  # Assign each point to the nearest polygon
  nearest_poly_index <- st_nearest_feature(xy, poly)
  xy_assigned <- xy
  xy_assigned$Name <- poly$Name[nearest_poly_index]
  xy_assigned$Area <- poly$Area[nearest_poly_index]
} else {
  stop("Unknown assignment method.")
}

# Drop polygons with no assigned points
used_names <- unique(na.omit(xy_assigned$Name))
poly <- poly %>% filter(Name %in% used_names)


 # Create grid table
grid <- xy_assigned %>%
  st_drop_geometry() %>%
  filter(Name %in% poly$Name) %>%
  select(SurveyAreaIdentifier, Name, Area) %>%
  distinct() %>%
  mutate(alpha_i = as.integer(factor(Name)))%>% 
  drop_na(Name) #Drop unassigned point

# Merge grid info back to events
events <- events %>%
  left_join(grid, by = "SurveyAreaIdentifier") %>%
  drop_na(Name)

# Visualization
  spatial_plot <- ggplot() +
    geom_sf(data = poly, aes(fill = Name), color = "black") +
    geom_sf(data = xy, color = "darkgrey", size = 2) +
    theme_minimal()
  
  print(spatial_plot)
  ggsave(file.path(plot.dir, "iCAR_Spatial_Plot.jpeg"), 
         plot = spatial_plot, width = 8, height = 6, dpi = 300)

```


### 2.1.2 Species or Guild Specific Analysis

For a species-specific analysis, select the species you wish to analyse.

```{r}

#To view your options of species
species<-unique(sp.data$CommonName)
#view(species)

#Create a list of species using all available species in the dataset.
species.list<-tolower("All") #R is case sensitive

#Or you can manually select species. Ensure case and spelling is correct. 
#species.list <- c("American Wigeon", "Common Loon", "Large Gull") 


```

For a guild-specific analysis, change `guild` to "Yes" and specify the `type` as either "migration", "diet", or "family". This will override the species list above.

*You do not need to run this code chunk if doing a species-specific analysis. Default guild is set to "No" in the setup script.*

```{r}

guild <- "yes"

#To view your options of guilds
migration<-unique(sp.data$Migration)
#view(migration)

diet<-unique(sp.data$Diet)
#view(diet)

family<-unique(sp.data$family_name)
#view(family)

#select on of the options for the analysis
type <-"migration"

```

### 2.1.3 Minimum Data Requirement

Select the minimum data requirements for the analysis. The ones selected here were done following the inspection of the International dataset. However, finer scale assessment may need to assess their suitability.

```{r}

#The minimum data required across sites with at least one detection:

min.abundance <- 10 #Overall abundance per year > 10
min.years <- (Y2-Y1)/2 #Detected in > one-half of the survey years
nsites <- 10 #Detected at > 10 survey sites

```

### 2.1.4 Model Specifications

Select the distributional family, and set random and spatial priors, or retain the defaults. These priors were selected based on model assessments for the full study area.

```{r}

#Here we select 'nbinomal' but this may need to adjust if there is residual overdispersion. 

fam<-'nbinomial'
#fam<-'poisson'

#Priors for the random effects
hyper.iid <- list(prec = list(prior = "pc.prec", param = c(1, 0.01)))

```

## 2.2 Analysis 

Create output tables to the `Output` folder using a custom file `name`.

The files that this code creates is a file for Annual Indices for each sampling site, end-point Trends, and a file with the Dispersion statistic.

You are ready to start your analysis! The analysis will write results to the files on the folder and also create some plots for model checking.

```{r}

#Give your analytically output file a unique name (e.g., BCCWS_Species, SalishSea_Migration, Watershed_Species), 

name<-"BCCWS_Species"

#This is the template used for the State of Canada's Birds and is required for upload into NatureCounts

#Now we can initiate the analysis based on the `model` you previously specified.   
run_analysis(model)

```

Note that the Dispersion Statistic file in the Output folder should be reviewed after the analysis. If the statistic is \> 10 you should inspect the FitPlots in the Plot directory. In this case we will want to rerun using a different distributional assumption on the counts. This can be done by manually changing the `fam` to Poission and selecting these species to be rerun.

## Map Results 

Finally, you can visualize your results.

```{r}

 graph_results(name)

```
